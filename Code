# Import libraries

import numpy as np 
import pandas as pd 
import re,string
import nltk 
import matplotlib.pyplot as plt
%matplotlib inline
import string
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer

gop = pd.read_csv('sentiment.csv')
tweet=gop['text']
sentiment=gop['sentiment']


# Tokenize + stopwords + Lemmatise

def strip_links(text):
    link_regex    = re.compile('((https?):((//)|(\\\\))+([\w\d:#@%/;$()~_?\+-=\\\.&](#!)?)*)', re.DOTALL)
    links         = re.findall(link_regex, text)
    for link in links:
        text = text.replace(link[0], ', ')    
    return text

def strip_all_entities(text):
    entity_prefixes = ['@','#']
    for separator in  string.punctuation:
        if separator not in entity_prefixes :
            text = text.replace(separator,' ')
    words = []
    for word in text.split():
        word = word.strip()
        if word:
            if word[0] not in entity_prefixes:
                words.append(word)
    return ' '.join(words)
    
tweet=tweet.str.replace('(RT)','')
tweet=tweet.apply(lambda x: strip_all_entities(strip_links(x)))
tokenizer=RegexpTokenizer(r"\w+")
newtweet=tweet.apply(lambda x: tokenizer.tokenize(x.lower()))

def remove_stopwords(text):
    words=[w for w in text if w not in stopwords.words('english')]
    return words
    
newtweet=newtweet.apply(lambda x: remove_stopwords(x))
lemmatizer=WordNetLemmatizer()

def word_lemmatizer(text):
    lem=" ".join([lemmatizer.lemmatize(i) for i in text])
    return lem
newtweet=newtweet.apply(lambda x: word_lemmatizer(x))
gop.text=newtweet
    
    
    
    
